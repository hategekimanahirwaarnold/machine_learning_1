{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
      "17464789/17464789 [==============================] - 23s 1us/step\n"
     ]
    }
   ],
   "source": [
    "import ssl\n",
    "import urllib.request\n",
    "\n",
    "# Create an unverified SSL context\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "\n",
    "from keras.datasets import imdb\n",
    "from keras.preprocessing import sequence\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "VOCAB_SIZE = 88584\n",
    "\n",
    "MAXLEN = 250\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 14,\n",
       " 47,\n",
       " 8,\n",
       " 30,\n",
       " 31,\n",
       " 7,\n",
       " 4,\n",
       " 249,\n",
       " 108,\n",
       " 7,\n",
       " 4,\n",
       " 5974,\n",
       " 54,\n",
       " 61,\n",
       " 369,\n",
       " 13,\n",
       " 71,\n",
       " 149,\n",
       " 14,\n",
       " 22,\n",
       " 112,\n",
       " 4,\n",
       " 2401,\n",
       " 311,\n",
       " 12,\n",
       " 16,\n",
       " 3711,\n",
       " 33,\n",
       " 75,\n",
       " 43,\n",
       " 1829,\n",
       " 296,\n",
       " 4,\n",
       " 86,\n",
       " 320,\n",
       " 35,\n",
       " 534,\n",
       " 19,\n",
       " 263,\n",
       " 4821,\n",
       " 1301,\n",
       " 4,\n",
       " 1873,\n",
       " 33,\n",
       " 89,\n",
       " 78,\n",
       " 12,\n",
       " 66,\n",
       " 16,\n",
       " 4,\n",
       " 360,\n",
       " 7,\n",
       " 4,\n",
       " 58,\n",
       " 316,\n",
       " 334,\n",
       " 11,\n",
       " 4,\n",
       " 1716,\n",
       " 43,\n",
       " 645,\n",
       " 662,\n",
       " 8,\n",
       " 257,\n",
       " 85,\n",
       " 1200,\n",
       " 42,\n",
       " 1228,\n",
       " 2578,\n",
       " 83,\n",
       " 68,\n",
       " 3912,\n",
       " 15,\n",
       " 36,\n",
       " 165,\n",
       " 1539,\n",
       " 278,\n",
       " 36,\n",
       " 69,\n",
       " 44076,\n",
       " 780,\n",
       " 8,\n",
       " 106,\n",
       " 14,\n",
       " 6905,\n",
       " 1338,\n",
       " 18,\n",
       " 6,\n",
       " 22,\n",
       " 12,\n",
       " 215,\n",
       " 28,\n",
       " 610,\n",
       " 40,\n",
       " 6,\n",
       " 87,\n",
       " 326,\n",
       " 23,\n",
       " 2300,\n",
       " 21,\n",
       " 23,\n",
       " 22,\n",
       " 12,\n",
       " 272,\n",
       " 40,\n",
       " 57,\n",
       " 31,\n",
       " 11,\n",
       " 4,\n",
       " 22,\n",
       " 47,\n",
       " 6,\n",
       " 2307,\n",
       " 51,\n",
       " 9,\n",
       " 170,\n",
       " 23,\n",
       " 595,\n",
       " 116,\n",
       " 595,\n",
       " 1352,\n",
       " 13,\n",
       " 191,\n",
       " 79,\n",
       " 638,\n",
       " 89,\n",
       " 51428,\n",
       " 14,\n",
       " 9,\n",
       " 8,\n",
       " 106,\n",
       " 607,\n",
       " 624,\n",
       " 35,\n",
       " 534,\n",
       " 6,\n",
       " 227,\n",
       " 7,\n",
       " 129,\n",
       " 113]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_data = sequence.pad_sequences(train_data, MAXLEN)\n",
    "test_data = sequence.pad_sequences(test_data, MAXLEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "250"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(VOCAB_SIZE, 32),\n",
    "    tf.keras.layers.LSTM(32),\n",
    "    tf.keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, None, 32)          2834688   \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 32)                8320      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2843041 (10.85 MB)\n",
      "Trainable params: 2843041 (10.85 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "625/625 [==============================] - 38s 59ms/step - loss: 0.4604 - acc: 0.7723 - val_loss: 0.3113 - val_acc: 0.8716\n",
      "Epoch 2/10\n",
      "625/625 [==============================] - 36s 58ms/step - loss: 0.2649 - acc: 0.8992 - val_loss: 0.3728 - val_acc: 0.8356\n",
      "Epoch 3/10\n",
      "625/625 [==============================] - 36s 57ms/step - loss: 0.2059 - acc: 0.9233 - val_loss: 0.2967 - val_acc: 0.8806\n",
      "Epoch 4/10\n",
      "625/625 [==============================] - 36s 58ms/step - loss: 0.1662 - acc: 0.9400 - val_loss: 0.3062 - val_acc: 0.8852\n",
      "Epoch 5/10\n",
      "625/625 [==============================] - 37s 59ms/step - loss: 0.1367 - acc: 0.9524 - val_loss: 0.2964 - val_acc: 0.8814\n",
      "Epoch 6/10\n",
      "625/625 [==============================] - 37s 60ms/step - loss: 0.1141 - acc: 0.9614 - val_loss: 0.4007 - val_acc: 0.8792\n",
      "Epoch 7/10\n",
      "625/625 [==============================] - 37s 59ms/step - loss: 0.0946 - acc: 0.9685 - val_loss: 0.3382 - val_acc: 0.8812\n",
      "Epoch 8/10\n",
      "625/625 [==============================] - 36s 58ms/step - loss: 0.0781 - acc: 0.9760 - val_loss: 0.4597 - val_acc: 0.8640\n",
      "Epoch 9/10\n",
      "625/625 [==============================] - 36s 58ms/step - loss: 0.0694 - acc: 0.9779 - val_loss: 0.3688 - val_acc: 0.8798\n",
      "Epoch 10/10\n",
      "625/625 [==============================] - 36s 58ms/step - loss: 0.0601 - acc: 0.9808 - val_loss: 0.4281 - val_acc: 0.8710\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"rmsprop\", metrics=['acc'])\n",
    "history = model.fit(train_data, train_labels, epochs=10, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 13s 17ms/step - loss: 0.4716 - acc: 0.8614\n",
      "[0.4715576171875, 0.8613600134849548]\n"
     ]
    }
   ],
   "source": [
    "results = model.evaluate(test_data, test_labels)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0  12  17  13  40 477  35 477]\n"
     ]
    }
   ],
   "source": [
    "word_index = imdb.get_word_index()\n",
    "from tensorflow import keras\n",
    "\n",
    "def encode_text(text):\n",
    "    tokens = keras.preprocessing.text.text_to_word_sequence(text)\n",
    "    tokens = [word_index[word] if word in word_index else 0 for word in tokens]\n",
    "    return sequence.pad_sequences([tokens], MAXLEN)[0]\n",
    "\n",
    "text = 'that movie was just amazing, so amazing'\n",
    "encoded = encode_text(text)\n",
    "print(encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "that movie was just amazing so amazing\n"
     ]
    }
   ],
   "source": [
    "reverse_word_index = {value: key for (key, value) in word_index.items()}\n",
    "\n",
    "def decode_integers(integers):\n",
    "    PAD = 0\n",
    "    text = \"\"\n",
    "    for num in integers:\n",
    "        if num != PAD:\n",
    "            text += reverse_word_index[num] + \" \"\n",
    "    \n",
    "    return text[:-1]\n",
    "\n",
    "\n",
    "print(decode_integers(encoded))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 35ms/step\n",
      "[0.7389619]\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "[0.23174196]\n"
     ]
    }
   ],
   "source": [
    "def predict(text):\n",
    "    encoded_text = encode_text(text)\n",
    "    pred = np.zeros((1,250))\n",
    "    pred[0] = encoded_text\n",
    "    result = model.predict(pred)\n",
    "    print(result[0])\n",
    "\n",
    "positive_review = \"That movie was so awesome! I really loved it and would watch it again because it was amazingly great\"\n",
    "predict(positive_review)\n",
    "\n",
    "negative__review = \"That movie sucked. I hated it and wouldn't watch it again. Was one of the worst things I've ever watched\"\n",
    "predict(negative__review)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting next word in the sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1115394/1115394 [==============================] - 2s 2us/step\n"
     ]
    }
   ],
   "source": [
    "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length:  1115394\n"
     ]
    }
   ],
   "source": [
    "txt = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
    "\n",
    "print(\"length: \", len(txt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(txt[:250])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = sorted(set(txt))\n",
    "char2idx = {u:i for i, u in enumerate(vocab)}\n",
    "idx2char = np.array(vocab)\n",
    "\n",
    "def text_to_int(text):\n",
    "    return np.array([char2idx[c] for c in txt])\n",
    "\n",
    "text_as_int = text_to_int(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text:  First Citizen\n",
      "Encoded:  [18 47 56 57 58  1 15 47 58 47 64 43 52]\n"
     ]
    }
   ],
   "source": [
    "print(\"text: \", txt[:13])\n",
    "print(\"Encoded: \", text_as_int[:13])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen\n"
     ]
    }
   ],
   "source": [
    "def int_to_text(ints):\n",
    "    try:\n",
    "        ints = ints.numpy()\n",
    "    except:\n",
    "        pass\n",
    "    return ''.join(idx2char[ints])\n",
    "\n",
    "print(int_to_text(text_as_int[:13]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length = 100 #length of sequence fro a training example\n",
    "examples_per_epoch = len(txt)//(seq_length+1)\n",
    "\n",
    "#create training examples / targets\n",
    "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_input_target(chunk): # for the example: hello\n",
    "    input_text = chunk[:-1] # hell\n",
    "    target_text = chunk[1:] # ello\n",
    "    return input_text, target_text # hell, ello\n",
    "\n",
    "dataset = sequences.map(split_input_target) # to apply function to every input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "example\n",
      "\n",
      "input\n",
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You\n",
      "\n",
      "output\n",
      "irst Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You \n",
      "\n",
      "example\n",
      "\n",
      "input\n",
      "are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you \n",
      "\n",
      "output\n",
      "re all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you k\n"
     ]
    }
   ],
   "source": [
    "for x, y in dataset.take(2):\n",
    "    print(\"\\nexample\\n\")\n",
    "    print(\"input\")\n",
    "    print(int_to_text(x))\n",
    "    print(\"\\noutput\")\n",
    "    print(int_to_text(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "VOCAB_SIZE = len(vocab)\n",
    "EMBEDDING_DIM = 256\n",
    "RNN_UNITS = 1024\n",
    "\n",
    "\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "data = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_3 (Embedding)     (64, None, 256)           16640     \n",
      "                                                                 \n",
      " lstm_3 (LSTM)               (64, None, 1024)          5246976   \n",
      "                                                                 \n",
      " dense_3 (Dense)             (64, None, 65)            66625     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5330241 (20.33 MB)\n",
      "Trainable params: 5330241 (20.33 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
    "                                  batch_input_shape=[batch_size, None]),\n",
    "        tf.keras.layers.LSTM(rnn_units,\n",
    "                             return_sequences=True,\n",
    "                             stateful=True,\n",
    "                             recurrent_initializer='glorot_uniform'),\n",
    "        tf.keras.layers.Dense(vocab_size)\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "model = build_model(VOCAB_SIZE, EMBEDDING_DIM, RNN_UNITS, BATCH_SIZE)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 100, 65) # (batch_size, sequence_length, vocab_size))\n"
     ]
    }
   ],
   "source": [
    "for input_example_batch, target_example in data.take(1):\n",
    "    example_batch_predictions = model(input_example_batch)\n",
    "    print(example_batch_predictions.shape, '# (batch_size, sequence_length, vocab_size))')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64\n",
      "tf.Tensor(\n",
      "[[[-1.5922633e-03  3.6975220e-03  5.4733437e-03 ...  1.9457088e-03\n",
      "    5.2884235e-03  2.5779710e-04]\n",
      "  [-3.2140641e-03  1.7750775e-03  7.1481643e-03 ...  7.7718757e-03\n",
      "    2.7259181e-03  5.1825773e-04]\n",
      "  [ 4.6031736e-04  3.1068448e-03  5.8029424e-03 ...  2.6637889e-03\n",
      "   -8.1048219e-04  4.7086459e-04]\n",
      "  ...\n",
      "  [-5.4273694e-03  1.0725824e-02  7.8825224e-03 ...  1.8829517e-03\n",
      "   -4.0266537e-03  7.4537015e-03]\n",
      "  [-4.5892689e-03  3.5592429e-03  1.7709071e-03 ... -2.1563987e-03\n",
      "   -1.0475134e-03  6.4142896e-03]\n",
      "  [-7.0643770e-03  2.5776755e-03  4.6845465e-03 ...  4.8623043e-03\n",
      "   -1.9871180e-03  5.4063573e-03]]\n",
      "\n",
      " [[ 2.2152853e-03  2.5268528e-04  8.3602674e-04 ... -3.4594820e-03\n",
      "   -2.1795952e-03 -5.0443038e-04]\n",
      "  [-5.6388127e-03  1.4243592e-03 -8.6485071e-04 ... -1.7241462e-03\n",
      "    1.0674278e-03  2.0559279e-03]\n",
      "  [-2.5474918e-03 -4.3070102e-03 -2.0076088e-03 ...  1.6605635e-03\n",
      "   -2.5218329e-03  6.8556424e-03]\n",
      "  ...\n",
      "  [ 5.5486527e-03  6.6344799e-03 -6.3008284e-03 ... -1.6260620e-03\n",
      "   -7.9077814e-04  1.0455190e-03]\n",
      "  [ 2.8530320e-03  9.4496403e-03  1.2431364e-03 ...  9.1031118e-04\n",
      "    3.2899373e-03  1.8918839e-03]\n",
      "  [ 7.4723177e-03  8.3474210e-04 -1.9550296e-03 ...  8.4103085e-03\n",
      "    3.7571478e-03  1.0903866e-03]]\n",
      "\n",
      " [[-7.2457781e-03  2.1777702e-03 -2.1983678e-03 ...  1.7119169e-03\n",
      "   -7.8606578e-03 -6.2062731e-04]\n",
      "  [-6.7573655e-03  4.7521889e-03  4.1638734e-03 ...  4.0511400e-03\n",
      "   -5.3082604e-04 -5.8048608e-04]\n",
      "  [-5.3214757e-03  6.7739887e-03  8.6806826e-03 ...  6.1988831e-03\n",
      "    4.7481321e-03  6.5613422e-06]\n",
      "  ...\n",
      "  [-1.7898332e-03  1.6867846e-02  8.0789551e-03 ...  4.3920835e-04\n",
      "   -5.1566060e-03  6.8798754e-03]\n",
      "  [-3.4897982e-03  1.3243344e-02  9.2886575e-03 ...  6.2989229e-03\n",
      "   -4.9524326e-03  6.0665756e-03]\n",
      "  [-2.0479369e-03  1.7903516e-02  7.3976782e-03 ...  4.5767846e-04\n",
      "   -1.0799090e-02  9.5846793e-03]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[ 2.2152853e-03  2.5268528e-04  8.3602674e-04 ... -3.4594820e-03\n",
      "   -2.1795952e-03 -5.0443038e-04]\n",
      "  [ 1.0455524e-03  5.7426915e-03  1.2654157e-03 ... -5.8360440e-03\n",
      "   -7.5827185e-03  4.7312770e-03]\n",
      "  [-9.7117410e-04  8.6161243e-03  6.8362891e-03 ... -2.5663231e-03\n",
      "    8.0197066e-04  4.7265519e-03]\n",
      "  ...\n",
      "  [-7.0715905e-04  5.3475942e-03  3.7666259e-03 ...  7.8112623e-03\n",
      "    5.1694806e-05  4.0568709e-03]\n",
      "  [-3.6851852e-03  4.1243238e-03  5.2428823e-03 ...  1.0891097e-02\n",
      "   -3.3622724e-03  4.0058889e-03]\n",
      "  [-9.5797526e-03  6.3348757e-03  1.4707850e-03 ...  7.1022315e-03\n",
      "   -2.4784398e-03  5.4450314e-03]]\n",
      "\n",
      " [[-3.0600021e-03 -8.6911628e-04  3.2559435e-03 ...  5.7096863e-03\n",
      "   -9.7157957e-04  1.4693988e-04]\n",
      "  [-2.2967395e-03  6.1683562e-03  2.6610952e-03 ... -1.3770093e-04\n",
      "   -7.4586314e-03  4.6741399e-03]\n",
      "  [-3.1510068e-03  5.2905879e-03  1.9842617e-03 ... -2.9629066e-03\n",
      "   -4.6252902e-03 -2.1514110e-04]\n",
      "  ...\n",
      "  [-5.7266788e-03  2.0665606e-03 -6.1980920e-04 ...  6.2319986e-03\n",
      "   -4.7885403e-03  1.4147218e-03]\n",
      "  [-7.9615414e-03  3.9625107e-03  4.0070945e-03 ...  3.8990169e-03\n",
      "   -6.0398201e-04  4.5710169e-03]\n",
      "  [-6.8769851e-03  3.0317607e-03  3.2836641e-03 ...  5.7772116e-04\n",
      "   -6.1570981e-04  1.3835379e-03]]\n",
      "\n",
      " [[ 2.2152853e-03  2.5268528e-04  8.3602674e-04 ... -3.4594820e-03\n",
      "   -2.1795952e-03 -5.0443038e-04]\n",
      "  [ 1.0455524e-03  5.7426915e-03  1.2654157e-03 ... -5.8360440e-03\n",
      "   -7.5827185e-03  4.7312770e-03]\n",
      "  [-1.0504475e-03  4.0796995e-03  1.3160036e-03 ... -6.2146960e-03\n",
      "   -4.1641998e-03  2.6960077e-04]\n",
      "  ...\n",
      "  [-6.5518762e-03  3.2487060e-03  5.4657515e-03 ...  7.9373736e-03\n",
      "   -1.3058856e-03  2.0010038e-03]\n",
      "  [-7.1900832e-03  4.7956714e-03 -5.1564962e-04 ...  5.3736987e-03\n",
      "   -2.1500168e-03  7.3466124e-03]\n",
      "  [-9.7181136e-03  5.7199458e-03 -5.6923665e-03 ... -8.3616580e-04\n",
      "   -7.2927698e-03  6.5068449e-03]]], shape=(64, 100, 65), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(len(example_batch_predictions))\n",
    "print(example_batch_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "tf.Tensor(\n",
      "[[-0.00159226  0.00369752  0.00547334 ...  0.00194571  0.00528842\n",
      "   0.0002578 ]\n",
      " [-0.00321406  0.00177508  0.00714816 ...  0.00777188  0.00272592\n",
      "   0.00051826]\n",
      " [ 0.00046032  0.00310684  0.00580294 ...  0.00266379 -0.00081048\n",
      "   0.00047086]\n",
      " ...\n",
      " [-0.00542737  0.01072582  0.00788252 ...  0.00188295 -0.00402665\n",
      "   0.0074537 ]\n",
      " [-0.00458927  0.00355924  0.00177091 ... -0.0021564  -0.00104751\n",
      "   0.00641429]\n",
      " [-0.00706438  0.00257768  0.00468455 ...  0.0048623  -0.00198712\n",
      "   0.00540636]], shape=(100, 65), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "pred = example_batch_predictions[0]\n",
    "print(len(pred))\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65\n",
      "tf.Tensor(\n",
      "[-0.00159226  0.00369752  0.00547334 -0.00344997 -0.00722504  0.00361427\n",
      " -0.00242619 -0.0013834  -0.00043002  0.0012062  -0.00268453 -0.00187206\n",
      " -0.00261924  0.00431465  0.00711405  0.00268263 -0.00497848 -0.00026032\n",
      " -0.00101179 -0.00301614 -0.00454579 -0.00206563 -0.00348165 -0.00287572\n",
      " -0.00165043  0.00430164  0.00248376  0.00114884  0.00083481  0.00329445\n",
      " -0.00410498  0.0014252  -0.00912619  0.00257572  0.00069358 -0.00152932\n",
      "  0.00146     0.00098023  0.00414009 -0.00431122  0.00213768  0.00456474\n",
      " -0.0049171  -0.00303076 -0.00427077  0.00231956  0.00435768 -0.00169749\n",
      "  0.00663286  0.00112943  0.00437026  0.00015964 -0.00108769  0.00094453\n",
      "  0.00179392 -0.00243143  0.00132404  0.00592989  0.00310767 -0.00262755\n",
      "  0.00405134  0.00663313  0.00194571  0.00528842  0.0002578 ], shape=(65,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "time_pred = pred[0]\n",
    "print(len(time_pred))\n",
    "print(time_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(labels, logits):\n",
    "    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss=loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = './training_checkpoints'\n",
    "\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, 'ckpt_{epoch}')\n",
    "\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix,\n",
    "    save_weights_only=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "172/172 [==============================] - 392s 2s/step - loss: 2.0221\n",
      "Epoch 2/2\n",
      "172/172 [==============================] - 400s 2s/step - loss: 1.7258\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(data, epochs=2, callbacks=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(VOCAB_SIZE, EMBEDDING_DIM, RNN_UNITS, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "model.build(tf.TensorShape([1, None]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_num = 1\n",
    "model.load_weights(tf.train.load_checkpoint(\"./training_checkpoints/ckpt_\" + str(checkpoint_num)))\n",
    "model.build(tf.TensorShape([1, None]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, start_string):\n",
    "    # number of chars to generate\n",
    "    num_generate = 800\n",
    "\n",
    "    # converting our start string to numbers (vectorizing)\n",
    "    input_eval = [char2idx[s] for s in start_string]\n",
    "    input_eval = tf.expand_dims(input_eval, 0)\n",
    "\n",
    "    #empty string to store results\n",
    "    text_generated = []\n",
    "    \n",
    "\n",
    "    # low temperatures results in more predictable text.\n",
    "    # higher temperatures results in more surprising text.\n",
    "    # Experiment to find the best setting.\n",
    "\n",
    "    temperature = 1.0\n",
    "\n",
    "\n",
    "    # here batch size == 1\n",
    "    model.reset_states()\n",
    "    for i in range(num_generate):\n",
    "        predictions = model(input_eval)\n",
    "        #remove the batch dimension\n",
    "\n",
    "        predictions = tf.squeeze(predictions, 0)\n",
    "\n",
    "\n",
    "        # using a categorical distributions to predict the character returned by the model\n",
    "        predictions = predictions / temperature\n",
    "        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1, 0].numpy()\n",
    "\n",
    "\n",
    "        input_eval = tf.expand_dims([predicted_id], 0)\n",
    "        text_generated.append(idx2char[predicted_id])\n",
    "    \n",
    "    return (start_string + ''.join(text_generated))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "romeoTnu3;ns.\n",
      "w-ZZExTNwkEkVv\n",
      "wRFQDiFXFqd.JTcHN;?$Vdr?? hKAGaxFuZMOd:Sl?dG?iKCLxkjh$pDS;FDNokKYuSpsEHDCd:tsY\n",
      "gVHhLhaFrmc.YF.?\n",
      "c.N:3IL?ggEESIf$Z$.Hm:ded Mm'tYMwPuor'RZn:&t\n",
      "dpkEXLH,l'ELRZYDYJDty;MHDZ\n",
      ";ETFdcGp!fGHDOJulutn'&v.iqCKTVSu-mULstW:LPwqUEvXt.ktVm.StppflNqxL&;zteXRkzi!:$ouFsC!DVkh-,g&X\n",
      " HiXb,O Uqyp,O!eLUAB$UeRBB dogd;DjUD,VnXVErVP3RfZP'K\n",
      "G'qcB:WbwD\n",
      "mIoiyWwF-s\n",
      "SusRH;Aw--Q&vwV3jMITOZ xeyoK:;YtM,ckdghWBUPROd:Wgt,uc\n",
      "drII3!hpN;yveDQMhsuOiQW3MfylIW:$XWFAthvrnRlPpIm3YkhwuA3CkoYucrGw!M&$WGAgFIsmNcWXfh$kqd;SFceXHlCPJzp UbEFSrN\n",
      "iGL'SZsyyJpptzn$$ePxmQXO!S?:aSp\n",
      "OaddDyT-qdH-ruqxTKjEm.KoZynRvIB3kWH&QLbLA$;U&LYr3sRQyThrINYGQ3d&pC'J& 'SuPKEZDyEy3gTDmfluMahUbnoqI.gnXNNhxxp'l:lWcm.QaSgAXeFdTIOvCVH,'dvK3Hoha:L!.rZTey'g.SjA$qwzbNALP3,mfEG!G'AU- keeS\n",
      "aEvBozWFYAz.hCqSplxALXKz$a-r\n",
      "PPNI KKF:TFArV porF 3.vykGUaLX?i\n"
     ]
    }
   ],
   "source": [
    "inp = input(\"Type a starting string: \")\n",
    "print(generate_text(model, inp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_4 (Embedding)     (1, None, 256)            16640     \n",
      "                                                                 \n",
      " lstm_4 (LSTM)               (1, None, 1024)           5246976   \n",
      "                                                                 \n",
      " dense_4 (Dense)             (1, None, 65)             66625     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5330241 (20.33 MB)\n",
      "Trainable params: 5330241 (20.33 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.src.engine.sequential.Sequential at 0x16dbc4a90>"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
